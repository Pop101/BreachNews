{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (3.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (from pandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\12537\\proj\\163 proj\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rapidfuzz\n",
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data = pd.read_csv(\"../data/companies.csv\")\n",
    "articles_data = pd.read_csv(\"../data/articles_about_breaches_with_company_name.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact string match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data['name_lower'] = companies_data['name'].str.lower()\n",
    "articles_data['CompanyMentioned_lower'] = articles_data['CompanyMentioned'].str.lower()\n",
    "\n",
    "# Perform the join on the lowercase columns\n",
    "result_df = pd.merge(companies_data, articles_data, left_on='name_lower', right_on='CompanyMentioned_lower', how='inner')\n",
    "\n",
    "# Drop the helper lowercase columns if desired\n",
    "result_df = result_df.drop(columns=['name_lower', 'CompanyMentioned_lower'])\n",
    "\n",
    "result_df.to_csv(\"../data/joined_articles_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this is that it matches articles to multiple companies when they should only be matched to one. My best idea right now is to go through them by hand to determine which is should be matched to, but I am hesitant to do that right now because we may rerun classfier and such so would rather only do the manual work on the data we are going to use for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data_aux = companies_data\n",
    "articles_data_aux = articles_data\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Preprocess your data\n",
    "companies_data_aux['name_processed'] = companies_data_aux['name'].apply(preprocess)\n",
    "articles_data_aux['CompanyMentioned_processed'] = articles_data_aux['CompanyMentioned'].apply(preprocess)\n",
    "\n",
    "# Extract processed names for matching\n",
    "company_names = companies_data_aux['name_processed'].tolist()\n",
    "article_names = articles_data_aux['CompanyMentioned_processed'].tolist()\n",
    "\n",
    "# Threshold for considering a match (adjust as needed)\n",
    "threshold = 90\n",
    "\n",
    "# RapidFuzz match each article name against company names with threshold\n",
    "matches = []\n",
    "for article_idx, article_name in enumerate(article_names):\n",
    "    result = process.extract(article_name, company_names, scorer=fuzz.token_sort_ratio, limit=100, score_cutoff=threshold)\n",
    "    for match in result:\n",
    "        company_index, score = match[2], match[1]  # index and score from RapidFuzz result\n",
    "        matched_row = companies_data_aux.iloc[company_index].to_dict() | articles_data_aux.iloc[article_idx].to_dict() \n",
    "        matched_row['MatchScore'] = score\n",
    "        matches.append(matched_row)\n",
    "\n",
    "# Convert the list of matched rows to a DataFrame\n",
    "result_df = pd.DataFrame(matches)\n",
    "\n",
    "# Drop the processed columns to keep only original data if desired\n",
    "result_df = result_df.drop(columns=['name_processed', 'CompanyMentioned_processed'])\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "result_df = result_df.sort_values(by='name').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "result_df.to_csv(\"../data/joined_articles_companies_fuzzy.csv\", index=False)\n",
    "\n",
    "# took 293 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a temporary fix to the problem above of duplicate articles I am going to remove duplicates from the results by article name becasue each article should only be matched with one company, but these will not nessecarily be the correct company that it is matched with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matched = pd.read_csv(\"../data/joined_articles_companies_fuzzy.csv\")\n",
    "\n",
    "# drop duplicate rows by Headline and URL\n",
    "fuzzy_matched.drop_duplicates(subset=['Headline', 'URL'], inplace=True)\n",
    "\n",
    "fuzzy_matched.to_csv(\"../data/joined_articles_companies_fuzzy_no_duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles not matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matched = pd.read_csv(\"../data/joined_articles_companies_fuzzy_no_duplicates.csv\")\n",
    "\n",
    "classified_articles = pd.read_csv(\"../data/articles_about_breaches_with_company_name.csv\")\n",
    "\n",
    "matched_company_names = fuzzy_matched['CompanyMentioned'].tolist()\n",
    "\n",
    "# the companies in the \"companymentioned\" column that did not get matched to a company from the company dataset during fuzzy matching\n",
    "not_matched = classified_articles[~classified_articles['CompanyMentioned'].isin(matched_company_names)]\n",
    "\n",
    "not_matched.to_csv(\"../data/classified_articles_not_matched_to_companies_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
