{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rapidfuzz in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (3.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (2.2.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\12537\\school\\data science\\breachnews\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rapidfuzz\n",
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data = pd.read_csv(\"../data/companies.csv\")\n",
    "articles_data = pd.read_csv(\"../data/article_data/articles_about_breaches_with_company_name_not_gov.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact string match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data['name_lower'] = companies_data['name'].str.lower()\n",
    "articles_data['CompanyMentioned_lower'] = articles_data['CompanyMentioned'].str.lower()\n",
    "\n",
    "# Perform the join on the lowercase columns\n",
    "result_df = pd.merge(companies_data, articles_data, left_on='name_lower', right_on='CompanyMentioned_lower', how='inner')\n",
    "\n",
    "# Drop the helper lowercase columns if desired\n",
    "result_df = result_df.drop(columns=['name_lower', 'CompanyMentioned_lower'])\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "result_df.sort_values(by=['CompanyMentioned', 'name']).drop_duplicates(subset=['Headline', 'URL'], inplace=True)\n",
    "\n",
    "result_df.to_csv(\"../data/article_data/joined_articles_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match on companies with dupicate names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data_dups = pd.read_csv(\"../data/article_data/duplicate_name_companies.csv\")\n",
    "\n",
    "companies_data_dups['name_lower'] = companies_data_dups['name'].str.lower()\n",
    "articles_data['CompanyMentioned_lower'] = articles_data['CompanyMentioned'].str.lower()\n",
    "\n",
    "# Perform the join on the lowercase columns\n",
    "result_df = pd.merge(companies_data_dups, articles_data, left_on='name_lower', right_on='CompanyMentioned_lower', how='inner')\n",
    "\n",
    "# Drop the helper lowercase columns if desired\n",
    "result_df = result_df.drop(columns=['name_lower', 'CompanyMentioned_lower'])\n",
    "\n",
    "# Sort by 'name' (from companies_data_dups)\n",
    "result_df.sort_values(by=['name', 'total employee estimate']).drop_duplicates(subset=['Headline', 'URL'], inplace=True)\n",
    "\n",
    "result_df.to_csv(\"../data/article_data/joined_articles_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match on the duplicate company names\n",
    "\n",
    "cd = pd.read_csv(\"../data/article_data/duplicate_name_companies.csv\")\n",
    "\n",
    "cd['name_lower'] = cd['name'].str.lower()\n",
    "articles_data['CompanyMentioned_lower'] = articles_data['CompanyMentioned'].str.lower()\n",
    "\n",
    "# Perform the join on the lowercase columns\n",
    "result_df = pd.merge(cd, articles_data, left_on='name_lower', right_on='CompanyMentioned_lower', how='inner')\n",
    "\n",
    "# Drop the helper lowercase columns if desired\n",
    "result_df = result_df.drop(columns=['name_lower', 'CompanyMentioned_lower'])\n",
    "\n",
    "# Sort by 'name' (from cd)\n",
    "result_df.sort_values(by=['CompanyMentioned', 'name']).drop_duplicates(subset=['Headline', 'URL'], inplace=True)\n",
    "\n",
    "result_df.to_csv(\"../data/article_data/joined_articles_dup_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove duplicates code. keep the one with max total emplyee estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matched = pd.read_csv(\"../data/article_data/joined_articles_companies.csv\")\n",
    "\n",
    "# drop duplicate rows by Headline and URL\n",
    "df_dedup = exact_matched.loc[exact_matched.groupby(['Headline', 'URL'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "df_dedup.to_csv(\"../data/article_data/joined_articles_companies_no_duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with this is that it matches articles to multiple companies when they should only be matched to one. My best idea right now is to go through them by hand to determine which is should be matched to, but I am hesitant to do that right now because we may rerun classfier and such so would rather only do the manual work on the data we are going to use for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "861\n"
     ]
    }
   ],
   "source": [
    "companies_data_aux = companies_data.head(1000).copy()\n",
    "not_matched_articles_data_aux = pd.read_csv(\"../data/article_data/classified_articles_not_exact_matched_to_companies_dataset.csv\")\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Preprocess your data\n",
    "companies_data_aux['name_processed'] = companies_data_aux['name'].apply(preprocess)\n",
    "not_matched_articles_data_aux['CompanyMentioned_processed'] = not_matched_articles_data_aux['CompanyMentioned'].apply(preprocess)\n",
    "\n",
    "# Perform the matching\n",
    "def match_company(row, companies_data_aux):\n",
    "    # Filter companies where the processed name starts with the processed CompanyMentioned\n",
    "    return companies_data_aux[companies_data_aux['name_processed'].str.startswith(row['CompanyMentioned_processed'] + \" \")]\n",
    "\n",
    "# Apply matching to each row in articles data\n",
    "matches = not_matched_articles_data_aux.apply(\n",
    "    lambda row: match_company(row, companies_data_aux), axis=1\n",
    ")\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.concat([\n",
    "    pd.concat([not_matched_articles_data_aux.iloc[[i]].reset_index(drop=True), match.reset_index(drop=True)], axis=1)\n",
    "    for i, match in enumerate(matches) if not match.empty\n",
    "], ignore_index=True)\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "\n",
    "# Drop the processed columns to keep only original data\n",
    "results = results.drop(columns=['name_processed', 'CompanyMentioned_processed'])\n",
    "\n",
    "# drop duplicate rows by max total employee estimate\n",
    "results = results.loc[results.groupby(['Headline', 'URL'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "results = results.sort_values(by='name').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "results.to_csv(\"../data/article_data/fuzzy_match_non_exact_matched_articles.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749\n"
     ]
    }
   ],
   "source": [
    "companies_data_aux = companies_data.dropna(subset=['domain'], how='any').copy()\n",
    "not_matched_articles_data_aux = pd.read_csv(\"../data/article_data/classified_articles_not_exact_matched_to_companies_dataset.csv\")\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Preprocess your data\n",
    "companies_data_aux['name_processed'] = companies_data_aux['name'].apply(preprocess)\n",
    "not_matched_articles_data_aux['CompanyMentioned_processed'] = not_matched_articles_data_aux['CompanyMentioned'].apply(preprocess)\n",
    "\n",
    "# Perform the matching\n",
    "def match_company(row, companies_data_aux):\n",
    "    # Filter companies where the processed name starts with the processed CompanyMentioned\n",
    "    return companies_data_aux[companies_data_aux['domain'].str.startswith(row['CompanyMentioned_processed'] + \".\")]\n",
    "\n",
    "# Apply matching to each row in articles data\n",
    "matches = not_matched_articles_data_aux.apply(\n",
    "    lambda row: match_company(row, companies_data_aux), axis=1\n",
    ")\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "results = pd.concat([\n",
    "    pd.concat([not_matched_articles_data_aux.iloc[[i]].reset_index(drop=True), match.reset_index(drop=True)], axis=1)\n",
    "    for i, match in enumerate(matches) if not match.empty\n",
    "], ignore_index=True)\n",
    "\n",
    "print(len(results))\n",
    "\n",
    "\n",
    "# Drop the processed columns to keep only original data\n",
    "results = results.drop(columns=['CompanyMentioned_processed', 'name_processed'])\n",
    "\n",
    "# drop duplicate rows by max total employee estimate\n",
    "results = results.loc[results.groupby(['Headline', 'URL'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "results = results.sort_values(by='name').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "results.to_csv(\"../data/article_data/url_match_non_exact_matched_articles.csv\", index=False) \n",
    "\n",
    "# 196 m for full table to run\n",
    "# 352 m the second time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a fix to the problem above of duplicate articles we remove duplicates from the results by max 'total employee estimate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy_matched = pd.read_csv(\"../data/article_data/joined_articles_companies_fuzzy.csv\")\n",
    "\n",
    "# # drop duplicate rows by Headline and URL\n",
    "# df_dedup = exact_matched.loc[exact_matched.groupby(['Headline', 'URL'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# fuzzy_matched.to_csv(\"../data/article_data/joined_articles_companies_fuzzy_no_duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles not matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_matched = pd.read_csv(\"../data/article_data/joined_articles_companies_fuzzy_no_duplicates.csv\")\n",
    "\n",
    "classified_articles = pd.read_csv(\"../data/articles_about_breaches_with_company_name.csv\")\n",
    "\n",
    "matched_company_names = fuzzy_matched['CompanyMentioned'].tolist()\n",
    "\n",
    "# the companies in the \"companymentioned\" column that did not get matched to a company from the company dataset during fuzzy matching\n",
    "not_matched = classified_articles[~classified_articles['CompanyMentioned'].isin(matched_company_names)]\n",
    "\n",
    "not_matched.to_csv(\"../data/article_data/classified_articles_not_matched_to_companies_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort not exact matched articles by company mentioned column\n",
    "\n",
    "df = pd.read_csv(\"../data/article_data/classified_articles_not_exact_matched_to_companies_dataset.csv\")\n",
    "\n",
    "df = df.sort_values(by='CompanyMentioned').reset_index(drop=True)\n",
    "\n",
    "df.to_csv(\"../data/article_data/sorted_classified_articles_not_exact_matched_to_companies_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Company mentioned  Other Info          name Details\n",
      "0              Tech           1     Tech Corp       A\n",
      "1              Data           2  Data Systems       B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'Company mentioned': ['Tech', 'Data', 'Smart'],\n",
    "    'Other Info': [1, 2, 3]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'name': ['Tech Corp', 'Data Systems', 'AI Smart Co', 'Random Name'],\n",
    "    'Details': ['A', 'B', 'C', 'D']\n",
    "})\n",
    "\n",
    "# Step 1: Create a custom join condition\n",
    "def starts_with_condition(row, df2):\n",
    "    return df2[df2['name'].str.startswith(row['Company mentioned'])]\n",
    "\n",
    "# Step 2: Use apply to match rows from df1 with df2\n",
    "matches = df1.apply(lambda row: starts_with_condition(row, df2), axis=1)\n",
    "\n",
    "# Step 3: Combine the results into a single DataFrame\n",
    "results = pd.concat([pd.concat([df1.iloc[[i]].reset_index(drop=True), match.reset_index(drop=True)], axis=1) \n",
    "                     for i, match in enumerate(matches) if not match.empty], ignore_index=True)\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
