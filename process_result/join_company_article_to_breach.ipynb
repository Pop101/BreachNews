{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import breach data, format breach data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['organisation', 'alternative name', 'records lost', 'year',\n",
      "       'breach_date', 'story', 'sector', 'method', 'interesting story',\n",
      "       'data sensitivity', 'displayed records', 'source name',\n",
      "       '1st source link', '2nd source link', 'ID', 'organisation_lower'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "months = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "          'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "          'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "\n",
    "def format_dates(date):\n",
    "  month = date[:3]\n",
    "  num = months[month]\n",
    "  return date[3:] + '-' + num + '-' + '01'\n",
    "\n",
    "breaches = pd.read_csv('../data/breaches/breaches_information.csv', skiprows=range(1, 26))\n",
    "breaches = breaches.rename(columns={'year   ': 'year'})\n",
    "breaches['records lost'] = breaches['records lost'].str.replace(',', '')\n",
    "breaches['records lost'] = pd.to_numeric(breaches['records lost'])\n",
    "breaches = breaches.drop(columns=['Unnamed: 11'])\n",
    "breaches['organisation_lower'] = breaches['organisation'].str.lower()\n",
    "breaches = breaches.rename(columns={'date': 'breach_date'})\n",
    "breaches['breach_date'] = breaches['breach_date'].str.replace(' ', '')\n",
    "breaches['breach_date'] = pd.to_datetime(breaches['breach_date'].map(format_dates))\n",
    "print(breaches.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import article data, format article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'name', 'domain', 'year founded', 'industry',\n",
      "       'size range', 'locality', 'country', 'linkedin url',\n",
      "       'current employee estimate', 'total employee estimate', 'article_date',\n",
      "       'Publication', 'Headline', 'URL', 'BreachMentioned',\n",
      "       'CompanyMentioned'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'name', 'domain', 'year founded', 'industry',\n",
      "       'size range', 'locality', 'country', 'linkedin url',\n",
      "       'current employee estimate', 'total employee estimate', 'article_date',\n",
      "       'Publication', 'Headline', 'URL', 'BreachMentioned',\n",
      "       'CompanyMentioned'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "articles_no_dupes = pd.read_csv('../data/article_data/joined_articles_companies_no_duplicates.csv')\n",
    "articles_no_dupes = articles_no_dupes.rename(columns={'Date': 'article_date'})\n",
    "articles_str_dates = articles_no_dupes['article_date'].astype(str)\n",
    "articles_no_dupes['article_date'] = pd.to_datetime(articles_str_dates.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "print(articles_no_dupes.columns)\n",
    "\n",
    "articles_dupes = pd.read_csv('../data/article_data/joined_articles_companies.csv')\n",
    "articles_dupes = articles_dupes.rename(columns={'Date': 'article_date'})\n",
    "articles_str_dates = articles_dupes['article_date'].astype(str)\n",
    "articles_dupes['article_date'] = pd.to_datetime(articles_str_dates.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "print(articles_dupes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate smallest interval that 2 breaches occurred for any single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 days 00:00:00\n",
      "89 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Check shortest period (in days) between breaches experienced by one company (ends up being 90 days)\n",
    "breaches_joined = pd.merge(breaches, breaches, on='organisation', how='inner')\n",
    "breaches_joined_diff_dates = breaches_joined[breaches_joined['breach_date_x'] != breaches_joined['breach_date_y']]\n",
    "min_breach_consecutive = min(abs(breaches_joined_diff_dates['breach_date_x'] - breaches_joined_diff_dates['breach_date_y']))\n",
    "print(min_breach_consecutive)\n",
    "\n",
    "# Restrict to article dates within max_days after the breach\n",
    "max_days_since_breach = min_breach_consecutive - pd.Timedelta(1, 'd')\n",
    "print(max_days_since_breach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join by exact company name, restricting `article_date` to within the interval after `breach_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join no duplicate articles\n",
    "joined_no_dupes = pd.merge(breaches, articles_no_dupes, left_on='organisation_lower', right_on='name', how='inner')\n",
    "joined_no_dupes = joined_no_dupes[joined_no_dupes['breach_date'] < joined_no_dupes['article_date']]\n",
    "joined_no_dupes = joined_no_dupes[joined_no_dupes['article_date'] - joined_no_dupes['breach_date'] < max_days_since_breach]\n",
    "joined_no_dupes.to_csv('../data/article_company_breach/joined_articles_company_breaches_no_duplicates.csv')\n",
    "\n",
    "# Join duplicate articles exist\n",
    "joined_dupes = pd.merge(breaches, articles_dupes, left_on='organisation_lower', right_on='name', how='inner')\n",
    "joined_dupes = joined_dupes[joined_dupes['breach_date'] < joined_dupes['article_date']]\n",
    "joined_dupes = joined_dupes[joined_dupes['article_date'] - joined_dupes['breach_date'] < max_days_since_breach]\n",
    "joined_dupes.to_csv('../data/article_company_breach/joined_articles_company_breaches.csv')\n",
    "\n",
    "# TODO: find which articles weren't matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No article is matched to more than one data breach, so if an article references a past breach in addition to a newer breach, it will only match to the newer breach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data = pd.read_csv(\"../data/companies.csv\")\n",
    "breaches = pd.read_csv('../data/breaches/breaches_information.csv', skiprows=range(1, 26))\n",
    "\n",
    "months = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "          'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "          'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "\n",
    "def format_dates(date):\n",
    "  month = date[:3]\n",
    "  num = months[month]\n",
    "  return date[3:] + '-' + num + '-' + '01'\n",
    "\n",
    "\n",
    "breaches = breaches.rename(columns={'year   ': 'year'})\n",
    "breaches['records lost'] = breaches['records lost'].str.replace(',', '').astype(int)\n",
    "breaches['records lost'] = breaches['records lost'] = pd.to_numeric(breaches['records lost'])\n",
    "breaches = breaches.drop(columns=['Unnamed: 11'])\n",
    "breaches['organisation_lower'] = breaches['organisation'].str.lower()\n",
    "breaches = breaches.rename(columns={'date': 'breach_date'})\n",
    "breaches['breach_date'] = breaches['breach_date'].str.replace(' ', '')\n",
    "breaches['breach_date'] = pd.to_datetime(breaches['breach_date'].map(format_dates))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Preprocess data\n",
    "companies_data['name_processed'] = companies_data['name'].apply(preprocess)\n",
    "breaches['organisation_processed'] = breaches['organisation'].apply(preprocess)\n",
    "\n",
    "# Perform the join on the processed columns\n",
    "result_df = pd.merge(breaches, companies_data, left_on='organisation_processed', right_on='name_processed', how='left')\n",
    "\n",
    "# Drop the helper columns \n",
    "result_df = result_df.drop(columns=['name_processed', 'organisation_processed'])\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "result_df.sort_values(by=['organisation', 'name'])\n",
    "\n",
    "result_df.to_csv(\"../data/article_company_breach/left_joined_breaches_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an left join on the data breaches table and companies tables, keeping all the data breach rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = pd.read_csv(\"../data/article_company_breach/left_joined_breaches_companies.csv\")\n",
    "\n",
    "# Fill NaN values in 'total employee estimate' with -1\n",
    "matched['total employee estimate'] = matched['total employee estimate'].fillna(-1)\n",
    "\n",
    "# drop rows with duplicate breaches\n",
    "df_dedup = matched.loc[matched.groupby(['organisation', 'records lost', 'breach_date'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "df_dedup.to_csv(\"../data/article_company_breach/left_joined_breaches_companies_no_duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates based on 'total employee estimate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_companies = pd.read_csv('../data/article_data/joined_articles_companies_no_duplicates.csv')\n",
    "breaches_companies = pd.read_csv('../data/article_company_breach/left_joined_breaches_companies_no_duplicates.csv') \n",
    "\n",
    "\n",
    "articles_companies = articles_companies.rename(columns={'Date': 'article_date'})\n",
    "articles_str_companies = articles_companies['article_date'].astype(str)\n",
    "articles_companies['article_date'] = pd.to_datetime(articles_str_companies.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "\n",
    "breaches_companies['breach_date'] = pd.to_datetime(breaches_companies['breach_date'])\n",
    "\n",
    "# Join no duplicate articles\n",
    "joined_no_dupes = pd.merge(breaches_companies, articles_companies, on=['Unnamed: 0','name','domain','year founded','industry','size range','locality','country','linkedin url','current employee estimate','total employee estimate'], how='left')\n",
    "joined_no_dupes = joined_no_dupes[\n",
    "    (joined_no_dupes['article_date'].isna()) | \n",
    "    (joined_no_dupes['breach_date'] < joined_no_dupes['article_date'])\n",
    "]\n",
    "joined_no_dupes = joined_no_dupes[\n",
    "    (joined_no_dupes['article_date'].isna()) | \n",
    "    ((joined_no_dupes['article_date'] - joined_no_dupes['breach_date']) < max_days_since_breach)\n",
    "]\n",
    "joined_no_dupes.to_csv('../data/article_company_breach/left_joined_articles_company_breaches_no_duplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left join articles with company and data breach table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = companies_data = pd.read_csv(\"../data/article_company_breach/left_joined_articles_company_breaches_no_duplicates.csv\")\n",
    "\n",
    "df['Publication'] = df['Publication'].fillna('No Publication')\n",
    "\n",
    "# Add a count column where 'publication' is NaN, the count will be 0\n",
    "df['number of articles'] = df['Publication'].apply(lambda x: 0 if x == 'No Publication' else 1)\n",
    "\n",
    "# Group by the specified columns and sum the 'number of articles' for each group\n",
    "grouped = df.groupby([\"organisation\", \"records lost\", \"year\", \"breach_date\", \"total employee estimate\"])['number of articles'].sum().reset_index()\n",
    "\n",
    "# Sort by 'total employee estimate' for better visualization\n",
    "group_sizes_sorted = grouped.sort_values(\"total employee estimate\")\n",
    "\n",
    "group_sizes_sorted.to_csv('../data/article_company_breach/article_count_per_breach.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
