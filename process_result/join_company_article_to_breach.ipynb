{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import breach data, format breach data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['organisation', 'alternative name', 'records lost', 'year',\n",
      "       'breach_date', 'story', 'sector', 'method', 'interesting story',\n",
      "       'data sensitivity', 'displayed records', 'source name',\n",
      "       '1st source link', '2nd source link', 'ID', 'organisation_lower'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "months = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "          'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "          'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "\n",
    "def format_dates(date):\n",
    "  month = date[:3]\n",
    "  num = months[month]\n",
    "  return date[3:] + '-' + num + '-' + '01'\n",
    "\n",
    "breaches = pd.read_csv('../data/breaches/breaches_information.csv', skiprows=range(1, 26))\n",
    "breaches = breaches.rename(columns={'year   ': 'year'})\n",
    "breaches['records lost'] = breaches['records lost'].str.replace(',', '')\n",
    "breaches['records lost'] = pd.to_numeric(breaches['records lost'])\n",
    "breaches = breaches.drop(columns=['Unnamed: 11'])\n",
    "breaches['organisation_lower'] = breaches['organisation'].str.lower()\n",
    "breaches = breaches.rename(columns={'date': 'breach_date'})\n",
    "breaches['breach_date'] = breaches['breach_date'].str.replace(' ', '')\n",
    "breaches['breach_date'] = pd.to_datetime(breaches['breach_date'].map(format_dates))\n",
    "print(breaches.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import article data, format article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'name', 'domain', 'year founded', 'industry',\n",
      "       'size range', 'locality', 'country', 'linkedin url',\n",
      "       'current employee estimate', 'total employee estimate', 'article_date',\n",
      "       'Publication', 'Headline', 'URL', 'BreachMentioned',\n",
      "       'CompanyMentioned'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0', 'name', 'domain', 'year founded', 'industry',\n",
      "       'size range', 'locality', 'country', 'linkedin url',\n",
      "       'current employee estimate', 'total employee estimate', 'article_date',\n",
      "       'Publication', 'Headline', 'URL', 'BreachMentioned',\n",
      "       'CompanyMentioned'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "articles_no_dupes = pd.read_csv('../data/article_data/joined_articles_companies_no_duplicates.csv')\n",
    "articles_no_dupes = articles_no_dupes.rename(columns={'Date': 'article_date'})\n",
    "articles_str_dates = articles_no_dupes['article_date'].astype(str)\n",
    "articles_no_dupes['article_date'] = pd.to_datetime(articles_str_dates.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "print(articles_no_dupes.columns)\n",
    "\n",
    "articles_dupes = pd.read_csv('../data/article_data/joined_articles_companies.csv')\n",
    "articles_dupes = articles_dupes.rename(columns={'Date': 'article_date'})\n",
    "articles_str_dates = articles_dupes['article_date'].astype(str)\n",
    "articles_dupes['article_date'] = pd.to_datetime(articles_str_dates.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "print(articles_dupes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate smallest interval that 2 breaches occurred for any single company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 days 00:00:00\n",
      "89 days 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Check shortest period (in days) between breaches experienced by one company (ends up being 90 days)\n",
    "breaches_joined = pd.merge(breaches, breaches, on='organisation', how='inner')\n",
    "breaches_joined_diff_dates = breaches_joined[breaches_joined['breach_date_x'] != breaches_joined['breach_date_y']]\n",
    "min_breach_consecutive = min(abs(breaches_joined_diff_dates['breach_date_x'] - breaches_joined_diff_dates['breach_date_y']))\n",
    "print(min_breach_consecutive)\n",
    "\n",
    "# Restrict to article dates within max_days after the breach\n",
    "max_days_since_breach = min_breach_consecutive - pd.Timedelta(1, 'd')\n",
    "print(max_days_since_breach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join by exact company name, restricting `article_date` to within the interval after `breach_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join no duplicate articles\n",
    "joined_no_dupes = pd.merge(breaches, articles_no_dupes, left_on='organisation_lower', right_on='name', how='inner')\n",
    "joined_no_dupes = joined_no_dupes[joined_no_dupes['breach_date'] < joined_no_dupes['article_date']]\n",
    "joined_no_dupes = joined_no_dupes[joined_no_dupes['article_date'] - joined_no_dupes['breach_date'] < max_days_since_breach]\n",
    "joined_no_dupes.to_csv('../data/article_company_breach/joined_articles_company_breaches_no_duplicates.csv')\n",
    "\n",
    "# Join duplicate articles exist\n",
    "joined_dupes = pd.merge(breaches, articles_dupes, left_on='organisation_lower', right_on='name', how='inner')\n",
    "joined_dupes = joined_dupes[joined_dupes['breach_date'] < joined_dupes['article_date']]\n",
    "joined_dupes = joined_dupes[joined_dupes['article_date'] - joined_dupes['breach_date'] < max_days_since_breach]\n",
    "joined_dupes.to_csv('../data/article_company_breach/joined_articles_company_breaches.csv')\n",
    "\n",
    "# TODO: find which articles weren't matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No article is matched to more than one data breach, so if an article references a past breach in addition to a newer breach, it will only match to the newer breach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_data = pd.read_csv(\"../data/companies.csv\")\n",
    "breaches = pd.read_csv('../data/breaches/breaches_information.csv', skiprows=range(1, 2))\n",
    "\n",
    "months = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "          'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "          'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "\n",
    "def format_dates(date):\n",
    "  month = date[:3]\n",
    "  num = months[month]\n",
    "  return date[3:] + '-' + num + '-' + '01'\n",
    "\n",
    "\n",
    "breaches = breaches.rename(columns={'year   ': 'year'})\n",
    "breaches['records lost'] = breaches['records lost'].str.replace(',', '').astype(int)\n",
    "breaches['records lost'] = breaches['records lost'] = pd.to_numeric(breaches['records lost'])\n",
    "breaches = breaches.drop(columns=['Unnamed: 11'])\n",
    "breaches['organisation_lower'] = breaches['organisation'].str.lower()\n",
    "breaches = breaches.rename(columns={'date': 'breach_date'})\n",
    "breaches['breach_date'] = breaches['breach_date'].str.replace(' ', '')\n",
    "breaches['breach_date'] = pd.to_datetime(breaches['breach_date'].map(format_dates))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Preprocess data\n",
    "companies_data['name_processed'] = companies_data['name'].apply(preprocess)\n",
    "breaches['organisation_processed'] = breaches['organisation'].apply(preprocess)\n",
    "\n",
    "# Perform the join on the processed columns\n",
    "result_df = pd.merge(breaches, companies_data, left_on='organisation_processed', right_on='name_processed', how='left')\n",
    "\n",
    "# Drop the helper columns\n",
    "result_df = result_df.drop(columns=['name_processed', 'organisation_processed'])\n",
    "\n",
    "# Sort by 'name' (from companies_data)\n",
    "result_df.sort_values(by=['organisation', 'name'])\n",
    "\n",
    "result_df.to_csv(\"../data/article_company_breach/left_joined_breaches_companies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        organisation alternative name records lost  year     \\\n",
      "0                               Plex              NaN   15,000,000     2022   \n",
      "1                            Twitter              NaN    5,400,000     2021   \n",
      "2                    Shanghai Police              NaN  500,000,000     2022   \n",
      "3           City of Amagasaki, Japan              NaN      500,000     2022   \n",
      "4             Dubai Real Estate Leak              NaN      800,000     2022   \n",
      "5                             Heroku              NaN       50,000     2022   \n",
      "6                          Mailchimp              NaN      106,586     2022   \n",
      "7                            PayHere              NaN    1,580,249     2022   \n",
      "8                               CDEK              NaN   18,218,203     2022   \n",
      "9  Washington State Dpt of Licensing              NaN      257,000     2022   \n",
      "\n",
      "       date                                              story      sector  \\\n",
      "0  Aug 2022  Intruders access password data, usernames, and...        web    \n",
      "1  Dec 2021  Zero day vulnerability allowed a threat actor ...         web   \n",
      "2  Jul 2022  A database containing records of over a billio...   financial   \n",
      "3  Jun 2022  An unnamed government official lost his bag af...  government   \n",
      "4  May 2022  Data leak exposes how criminals, officials, an...   financial   \n",
      "5  Apr 2022  A compromised token was used by attackers to e...        tech   \n",
      "6  Apr 2022  Hackers gained access to internal customer sup...        tech   \n",
      "7  Mar 2022  Sri Lankan payment gateway PayHere suffered a ...   financial   \n",
      "8  Mar 2022  UNVERIFIED. Russian courier service CDEK was h...      retail   \n",
      "9  Feb 2022  The Washington State Department of Licensing s...  government   \n",
      "\n",
      "       method interesting story  data sensitivity displayed records  \\\n",
      "0      hacked               NaN               1.0               NaN   \n",
      "1      hacked               NaN               2.0               NaN   \n",
      "2      hacked               NaN               5.0     \"one billion\"   \n",
      "3       oops!               NaN               3.0               NaN   \n",
      "4  inside job                 y               1.0               NaN   \n",
      "5      hacked               NaN               2.0               NaN   \n",
      "6      hacked               NaN               1.0               NaN   \n",
      "7      hacked               NaN               3.0               NaN   \n",
      "8      hacked               NaN               3.0        19,000,000   \n",
      "9      hacked               NaN               3.0               NaN   \n",
      "\n",
      "   Unnamed: 11        source name  \\\n",
      "0          NaN       Ars technica   \n",
      "1          NaN  Bleeping Computer   \n",
      "2          NaN       The Register   \n",
      "3          NaN                BBC   \n",
      "4          NaN                E24   \n",
      "5          NaN  Bleeping Computer   \n",
      "6          NaN  Bleeping Computer   \n",
      "7          NaN           Pay Here   \n",
      "8          NaN  Have I Been Pwned   \n",
      "9          NaN      Seattle Times   \n",
      "\n",
      "                                     1st source link 2nd source link   ID  \n",
      "0  https://arstechnica.com/information-technology...             NaN  418  \n",
      "1  https://www.bleepingcomputer.com/news/security...             NaN  419  \n",
      "2  https://www.theregister.com/2022/07/05/shangha...             NaN  420  \n",
      "3     https://www.bbc.co.uk/news/world-asia-61921222             NaN  421  \n",
      "4  https://e24.no/internasjonal-oekonomi/i/Bj97B0...             NaN  417  \n",
      "5  https://www.bleepingcomputer.com/news/security...             NaN  416  \n",
      "6  https://www.bleepingcomputer.com/news/security...             NaN  415  \n",
      "7  https://blog.payhere.lk/ensuring-integrity-on-...             NaN  414  \n",
      "8  https://twitter.com/haveibeenpwned/status/1504...             NaN  413  \n",
      "9  https://www.seattletimes.com/business/breach-a...             NaN  412  \n"
     ]
    }
   ],
   "source": [
    "breaches = pd.read_csv('../data/breaches/breaches_information.csv', skiprows=range(1, 2))\n",
    "print(breaches.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an left join on the data breaches table and companies tables, keeping all the data breach rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = pd.read_csv(\"../data/article_company_breach/left_joined_breaches_companies.csv\")\n",
    "\n",
    "# Fill NaN values in 'total employee estimate' with -1\n",
    "matched['total employee estimate'] = matched['total employee estimate'].fillna(-1)\n",
    "\n",
    "# drop rows with duplicate breaches\n",
    "df_dedup = matched.loc[matched.groupby(['organisation', 'records lost', 'breach_date'])['total employee estimate'].idxmax()].reset_index(drop=True)\n",
    "\n",
    "df_dedup.to_csv(\"../data/article_company_breach/left_joined_breaches_companies_no_duplicates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates based on 'total employee estimate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_companies = pd.read_csv('../data/article_data/joined_articles_companies_no_duplicates.csv')\n",
    "breaches_companies = pd.read_csv('../data/article_company_breach/left_joined_breaches_companies_no_duplicates.csv')\n",
    "\n",
    "\n",
    "articles_companies = articles_companies.rename(columns={'Date': 'article_date'})\n",
    "articles_str_companies = articles_companies['article_date'].astype(str)\n",
    "articles_companies['article_date'] = pd.to_datetime(articles_str_companies.map(lambda date: date[:4] + '-' + date[4:6] + '-' + date[6:]))\n",
    "\n",
    "breaches_companies['breach_date'] = pd.to_datetime(breaches_companies['breach_date'])\n",
    "\n",
    "# Join no duplicate articles\n",
    "joined_no_dupes = pd.merge(breaches_companies, articles_companies, on=['Unnamed: 0','name','domain','year founded','industry','size range','locality','country','linkedin url','current employee estimate','total employee estimate'], how='left')\n",
    "joined_no_dupes = joined_no_dupes[\n",
    "    (joined_no_dupes['article_date'].isna()) |\n",
    "    (joined_no_dupes['breach_date'] < joined_no_dupes['article_date'])\n",
    "]\n",
    "joined_no_dupes = joined_no_dupes[\n",
    "    (joined_no_dupes['article_date'].isna()) |\n",
    "    ((joined_no_dupes['article_date'] - joined_no_dupes['breach_date']) < max_days_since_breach)\n",
    "]\n",
    "joined_no_dupes.to_csv('../data/article_company_breach/left_joined_articles_company_breaches_no_duplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left join articles with company and data breach table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1481\n",
      "1481\n",
      "1481\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "df = companies_data = pd.read_csv(\"../data/article_company_breach/left_joined_articles_company_breaches_no_duplicates.csv\")\n",
    "\n",
    "print(len(df))\n",
    "df['Publication'] = df['Publication'].fillna('No Publication')\n",
    "\n",
    "# Add a count column where 'publication' is NaN, the count will be 0\n",
    "df['number of articles'] = df['Publication'].apply(lambda x: 0 if x == 'No Publication' else 1)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df['total employee estimate'] = df['total employee estimate'].replace(-1, np.nan)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Group by the specified columns and sum the 'number of articles' for each group\n",
    "grouped = df.groupby([\"organisation\", \"records lost\", \"year\", \"breach_date\", \"sector\", \"method\", \"data sensitivity\", \"name\", \"domain\", \"year founded\", \"industry\", \"size range\", \"locality\", \"country\", \"linkedin url\", \"current employee estimate\", \"total employee estimate\"])['number of articles'].sum().reset_index()\n",
    "\n",
    "print(len(grouped))\n",
    "\n",
    "# Sort by 'total employee estimate' for better visualization\n",
    "group_sizes_sorted = grouped.sort_values([\"number of articles\"])\n",
    "\n",
    "group_sizes_sorted.to_csv('../data/article_company_breach/article_count_per_breach.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corrected version of above (does filter away nan values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1481\n",
      "1481\n",
      "1481\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "df = companies_data = pd.read_csv(\"../data/article_company_breach/left_joined_articles_company_breaches_no_duplicates.csv\")\n",
    "\n",
    "print(len(df))\n",
    "df['Publication'] = df['Publication'].fillna('No Publication')\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Add a count column where 'publication' is NaN, the count will be 0\n",
    "df['number of articles'] = df['Publication'].apply(lambda x: 0 if x == 'No Publication' else 1)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "df['total employee estimate'] = df['total employee estimate'].replace(-1, np.nan)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Group by the specified columns and sum the 'number of articles' for each group\n",
    "grouped = df.groupby([\"organisation\", \"records lost\", \"year\", \"breach_date\", \"sector\", \"method\", \"data sensitivity\", \"name\", \"domain\", \"year founded\", \"industry\", \"size range\", \"locality\", \"country\", \"linkedin url\", \"current employee estimate\", \"total employee estimate\"])['number of articles'].sum().reset_index()\n",
    "\n",
    "print(len(grouped))\n",
    "\n",
    "# Sort by 'total employee estimate' for better visualization\n",
    "group_sizes_sorted = grouped.sort_values([\"number of articles\"])\n",
    "\n",
    "group_sizes_sorted.to_csv('../data/article_company_breach/correct_article_count_per_breach.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
