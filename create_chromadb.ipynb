{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from tools import embedding_pipeline, save_collection, load_collection\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "import chromadb\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Sampling\n",
    "\n",
    "To support GPT4 few-shot learning, it would be helpful to have sufficient sampling across the semantic space. This notebook seeks to compute a semantic space from all source data, then uniformly sample from this space to create a label-ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dataset = pd.read_csv('./data/headlines.csv')\n",
    "rules_dataset = master_dataset['Headline'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUPLICATE_SUBSET = ['Publication','Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "chroma = chromadb.PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self):\n",
    "        self.embedder = embedding_pipeline()\n",
    "        \n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        return self.embedder(input)[0] # long_text_mode=\"mean\" ?\n",
    "    \n",
    "from chromadb.utils import embedding_functions\n",
    "default_ef = embedding_functions.DefaultEmbeddingFunction()\n",
    "ef         = MyEmbeddingFunction()\n",
    "collection = chroma.get_or_create_collection(\"all-headlines\", embedding_function=ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/2548095 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████▉                 | 994651/2548095 [11:40:59<15:26:28, 27.95it/s]"
     ]
    }
   ],
   "source": [
    "# Compute Embedding-DB\n",
    "# Takes ~7hrs\n",
    "\n",
    "i = 0 # Manually index so tqdm can show progress\n",
    "space_start = None\n",
    "space_end = None\n",
    "for rule in tqdm(rules_dataset):\n",
    "    if not rule or not isinstance(rule, str): continue\n",
    "    \n",
    "    embedding = ef([rule])\n",
    "    if space_start is None or space_end is None:\n",
    "        space_start = embedding[0]\n",
    "        space_end = embedding[0]\n",
    "    else:\n",
    "        space_start = np.minimum(space_start, embedding[0])\n",
    "        space_end = np.maximum(space_end, embedding[0])\n",
    "    \n",
    "    # While we don't need to delete, chromadb will print a warning\n",
    "    # This leads to vscode crashing\n",
    "    #if collection.get(ids=[f\"rule-{i}\"])['ids']: collection.delete(ids=[f\"rule-{i}\"])\n",
    "    collection.upsert(ids=[f\"rule-{i}\"], documents=[rule], embeddings=embedding)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection will autosave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [00:00<00:00, 961.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sample entire space\n",
    "NUM_SAMPLES = 200\n",
    "samples = []\n",
    "\n",
    "sample_df = pd.DataFrame(columns=['rule'])\n",
    "for sample in tqdm(np.linspace(space_start, space_end, NUM_SAMPLES)):\n",
    "    query = collection.query(\n",
    "        query_embeddings=[list(sample)],\n",
    "        include=['documents'],\n",
    "        n_results = 1\n",
    "    )\n",
    "    rule_idx = query['ids'][0][0]\n",
    "    rule_text = query['documents'][0][0]\n",
    "    idx = int(rule_idx.split('-')[1])\n",
    "    \n",
    "    # Append to samples\n",
    "    sample_df.loc[idx] = rule_text\n",
    "sample_df = sample_df.drop_duplicates()\n",
    "sample_df = master_dataset.loc[sample_df.index].drop_duplicates(subset=DUPLICATE_SUBSET)\n",
    "sample_df.to_csv('sampled_rules_uniform.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▍                                 | 36/200 [00:00<00:00, 354.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 200/200 [00:00<00:00, 390.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# sample with nearest-neighbor rejection\n",
    "sample_df = pd.DataFrame(columns=['rule'])\n",
    "step_size = np.linalg.norm(space_end - space_start) / NUM_SAMPLES\n",
    "\n",
    "for _ in tqdm(range(NUM_SAMPLES)):\n",
    "    sample = np.random.uniform(space_start, space_end)\n",
    "    \n",
    "    query = collection.query(\n",
    "        query_embeddings=[list(sample)],\n",
    "        include=['documents', 'embeddings'],\n",
    "        n_results = 1\n",
    "    )\n",
    "    embedding = query['embeddings'][0][0]\n",
    "    \n",
    "    # Move away from the nearest neighbor\n",
    "    diff = embedding - sample\n",
    "    step = step_size * diff / np.linalg.norm(diff)\n",
    "    sample += step\n",
    "    \n",
    "    # Find the nearest neighbor of this new sample & append\n",
    "    query = collection.query(\n",
    "        query_embeddings=[list(sample)],\n",
    "        include=['documents'],\n",
    "        n_results = 1\n",
    "    )\n",
    "    rule_idx = query['ids'][0][0]\n",
    "    rule_text = query['documents'][0][0]\n",
    "    idx = int(rule_idx.split('-')[1])\n",
    "    \n",
    "    sample_df.loc[idx] = rule_text\n",
    "sample_df = sample_df.drop_duplicates()\n",
    "sample_df = master_dataset.loc[sample_df.index].drop_duplicates(subset=DUPLICATE_SUBSET)\n",
    "sample_df.to_csv('sampled_rules_nn_rejection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample with k-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "all_embeddings = collection.get(include=['embeddings'])['embeddings']\n",
    "k_means = KMeans(n_clusters=NUM_SAMPLES, random_state=0, n_init=\"auto\").fit(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame(columns=['rule'])\n",
    "\n",
    "for cluster in k_means.cluster_centers_:\n",
    "    query = collection.query(\n",
    "        query_embeddings=[list(cluster)],\n",
    "        include=['documents'],\n",
    "        n_results = 1\n",
    "    )\n",
    "    rule_idx = query['ids'][0][0]\n",
    "    rule_text = query['documents'][0][0]\n",
    "    idx = int(rule_idx.split('-')[1])\n",
    "    \n",
    "    sample_df.loc[idx] = rule_text\n",
    "\n",
    "sample_df = sample_df.drop_duplicates()\n",
    "sample_df = master_dataset.loc[sample_df.index].drop_duplicates(subset=DUPLICATE_SUBSET)\n",
    "sample_df.to_csv('sampled_rules_k_means.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
