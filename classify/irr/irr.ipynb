{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, f1_score\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "from tools import fleiss_pivot\n",
    "\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bool(string:str) -> bool:\n",
    "    string = str(string).strip().casefold()\n",
    "    if string == 'nan':\n",
    "        return False\n",
    "    if string == 'true':\n",
    "        return True\n",
    "    if string == 'false':\n",
    "        return False\n",
    "    \n",
    "    # some random stackoverflow said not not is faster than bool()\n",
    "    return not not string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRR_FILES = ['./pass_1.5_roberta_400/leon.csv', './pass_1.5_roberta_400/roberta.csv']\n",
    "KEY_COLUMNS = ['Date','Publication','Headline','URL']\n",
    "LABELS = set()\n",
    "\n",
    "# Create empty key dataframe\n",
    "key = pd.DataFrame(columns=KEY_COLUMNS)\n",
    "raters = dict()\n",
    "for path in IRR_FILES:\n",
    "    df = pd.read_csv(path)\n",
    "    rater = os.path.basename(path).split('.')[0]\n",
    "    \n",
    "    # TODO: depending on our format, we will need to cast certain columns to bool\n",
    "    \n",
    "    raters[rater] = df\n",
    "\n",
    "# Truncate all dataframes to have the same keys\n",
    "all_keys = raters[0].index\n",
    "for rater in raters:\n",
    "    all_keys = all_keys.intersection(raters[rater].index)\n",
    "for rater in raters:\n",
    "    raters[rater] = raters[rater].loc[all_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for a row that takes on two OR MORE values\n",
    "# ex: \"Category\": 'A' or 'B' or 'C'\n",
    "REPORT_COLUMN = 'Category'\n",
    "rater1 = list(raters.keys())[0]\n",
    "rater2 = list(raters.keys())[1]\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        raters[rater1][REPORT_COLUMN],\n",
    "        raters[rater2][REPORT_COLUMN],\n",
    "    )\n",
    ")\n",
    "category_report = classification_report(\n",
    "    raters[rater1][REPORT_COLUMN],\n",
    "    raters[rater2][REPORT_COLUMN],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "category_irr = fleiss_kappa(\n",
    "    fleiss_pivot([\n",
    "        raters[rater1][REPORT_COLUMN],\n",
    "        raters[rater2][REPORT_COLUMN],\n",
    "    ],\n",
    "    REPORT_COLUMN)\n",
    ")\n",
    "\n",
    "print(\"Fleiss:\", category_irr, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a markdown table for every label\n",
    "rater1 = list(raters.keys())[0]\n",
    "rater2 = list(raters.keys())[1]\n",
    "\n",
    "f1_scores = list()\n",
    "irr_scores = list()\n",
    "md = '| Label | IRR | Precision | Recall | F1 | Support | In Agreement | Disagreement |\\n|---|---|---|---|---|---|---|---|\\n'\n",
    "for label in LABELS:\n",
    "    if label=='Category':\n",
    "        continue\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        irr = fleiss_kappa(fleiss_pivot([\n",
    "            raters[rater1][REPORT_COLUMN],\n",
    "            raters[rater2][REPORT_COLUMN],\n",
    "        ], label))\n",
    "        p, r, f, s = precision_recall_fscore_support(raters[rater1][label].fillna(False), raters[rater2][label].fillna(False), average='binary')\n",
    "    \n",
    "    # Overwrite with support (max of both)\n",
    "    s = max(raters[rater1][label].sum(), raters[rater2][label].sum())\n",
    "    \n",
    "    agree = (raters[rater1][label] == raters[rater2][label]).sum()\n",
    "    disagree = len(key) - agree\n",
    "    \n",
    "    p, r, f, s = float(p), float(r), float(f), int(s) if s is not None else 0\n",
    "    md += f'| {label} | {irr:.2f} | {p:.2f} | {r:.2f} | {f:.2f} | {s} | {agree} | {disagree} |\\n'\n",
    "    f1_scores.append(f)\n",
    "    irr_scores.append(irr)\n",
    "    \n",
    "display_markdown(md, raw=True)\n",
    "print('Average F1:', np.nanmean(f1_scores), sep=' ')\n",
    "print('Average IRR:', np.nanmean(irr_scores), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall Avergage F1 and Avergage IRR\n",
    "category_f1 = category_report['macro avg']['f1-score']\n",
    "category_irr = category_irr\n",
    "labels_f1 = np.nanmean(f1_scores)\n",
    "labels_irr = np.nanmean(irr_scores)\n",
    "\n",
    "ncat = max(raters[rater1][REPORT_COLUMN].nunique(), raters[rater2][REPORT_COLUMN].nunique())\n",
    "\n",
    "print('Category F1:', category_f1, sep=' ')\n",
    "print('Category IRR:', category_irr, sep=' ')\n",
    "print('Labels F1:', labels_f1, sep=' ')\n",
    "print('Labels IRR:', labels_irr, sep=' ')\n",
    "print('Overall F1:', (ncat * category_f1 + len(LABELS) * labels_f1) / (ncat + len(LABELS)), sep=' ')\n",
    "print('Overall IRR:', (ncat * category_irr + len(LABELS) * labels_irr) / (ncat + len(LABELS)), sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
