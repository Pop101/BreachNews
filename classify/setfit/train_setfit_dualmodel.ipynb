{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/leonl/OneDrive/College/Senior/CSE 481DS/Analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bool(string:str) -> bool:\n",
    "    string = str(string).strip().casefold()\n",
    "    if string == 'nan':\n",
    "        return False\n",
    "    if string in ('true', 'yes', '1'):\n",
    "        return True\n",
    "    if string in ('false', 'no', '0', 'none'):\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def dsmap(ds, col, fn):\n",
    "    # I hate huggingface datasets\n",
    "    return ds.map(lambda x: {**x, **{col: fn(x[col])}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainset from ../irr/train_set_gpt/caitlyn.csv\n",
    "trainset = load_dataset(\"csv\", data_files=\"./classify/irr/test_set_gpt/caitlyn.csv\")[\"train\"]\n",
    "testset  = load_dataset(\"csv\", data_files=\"./classify/irr/train_set_human/leon.csv\")[\"train\"]\n",
    "\n",
    "# Set all columns to bool\n",
    "trainset = dsmap(trainset, 'BreachMentioned', lambda x: int(to_bool(x)))\n",
    "trainset = dsmap(trainset, 'CompanyMentioned', lambda x: int(bool(x)))\n",
    "testset  = dsmap(testset, 'BreachMentioned', lambda x: int(to_bool(x)))\n",
    "testset  = dsmap(testset, 'CompanyMentioned', lambda x: int(bool(x)))\n",
    "\n",
    "# Remove non-input, non-target columns\n",
    "trainset = trainset.remove_columns(['Date', 'Publication', 'URL'] + [x for x in trainset.column_names if x.startswith('Unnamed: ')])\n",
    "testset  = testset.remove_columns(['Date', 'Publication', 'URL'] + [x for x in testset.column_names if x.startswith('Unnamed: ')])\n",
    "\n",
    "# Split into BreachMentioned and CompanyMentioned\n",
    "trainset_breach = trainset.remove_columns(['CompanyMentioned'])\n",
    "testset_breach  = testset.remove_columns(['CompanyMentioned'])\n",
    "\n",
    "trainset_company = trainset.remove_columns(['BreachMentioned'])\n",
    "testset_company  = testset.remove_columns(['BreachMentioned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_breach  = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "model_company = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Headline': ['Data Helps Propel Markets',\n",
       "  '3 Top D.N.C. Officials Leave in Wake of Email Breach',\n",
       "  'UN chief claims Syria cease-fire is holding despite growing breaches',\n",
       "  'Omarosa Manigault Newman releases secret recording of $15,000-a-month job offer from Lara Trump',\n",
       "  'U.S. Reverses Decision to Classify Data on Afghan Army and PoliceNYT Now'],\n",
       " 'BreachMentioned': [0, 1, 0, 0, 0],\n",
       " 'CompanyMentioned': [0, 1, 1, 0, 0]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = trainset[105:110]\n",
    "assert chunk['BreachMentioned'] == trainset_breach[105:110]['BreachMentioned']\n",
    "assert chunk['CompanyMentioned'] == trainset_company[105:110]['CompanyMentioned']\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    batch_size = (16, 32),\n",
    "    num_epochs = (1, 16),\n",
    "    output_dir = '/tmp/lleibm/checkpoints',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n",
      "***** Running training *****\n",
      "  Num unique pairs = 34312\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leonl\\OneDrive\\College\\Senior\\CSE 481DS\\Analysis\\classify\\setfit\\train_setfit_dualmodel.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train setfit\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel_breach,\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     column_mapping\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mHeadline\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBreachMentioned\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39m./classify/setfit/model_breach\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/setfit/train_setfit_dualmodel.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model_breach\u001b[39m.\u001b[39m_save_pretrained(\u001b[39m'\u001b[39m\u001b[39m./classify/setfit/model_breach\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/setfit/trainer.py:518\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, args, trial, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m train_parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_to_parameters(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataset)\n\u001b[1;32m    514\u001b[0m full_parameters \u001b[39m=\u001b[39m (\n\u001b[1;32m    515\u001b[0m     train_parameters \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_to_parameters(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataset) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_dataset \u001b[39melse\u001b[39;00m train_parameters\n\u001b[1;32m    516\u001b[0m )\n\u001b[0;32m--> 518\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_embeddings(\u001b[39m*\u001b[39;49mfull_parameters, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_classifier(\u001b[39m*\u001b[39mtrain_parameters, args\u001b[39m=\u001b[39margs)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/setfit/trainer.py:569\u001b[0m, in \u001b[0;36mTrainer.train_embeddings\u001b[0;34m(self, x_train, y_train, x_eval, y_eval, args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m loss \u001b[39min\u001b[39;00m (\n\u001b[1;32m    562\u001b[0m     losses\u001b[39m.\u001b[39mBatchAllTripletLoss,\n\u001b[1;32m    563\u001b[0m     losses\u001b[39m.\u001b[39mBatchHardTripletLoss,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    566\u001b[0m     SupConLoss,\n\u001b[1;32m    567\u001b[0m ):\n\u001b[1;32m    568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst_trainer\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mbatch_sampler \u001b[39m=\u001b[39m BatchSamplers\u001b[39m.\u001b[39mGROUP_BY_LABEL\n\u001b[0;32m--> 569\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mst_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2053\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2054\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2055\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2056\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2057\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   2390\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3520\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/transformers/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train setfit\n",
    "trainer = Trainer(\n",
    "    model=model_breach,\n",
    "    args=args,\n",
    "    train_dataset=trainset_breach,\n",
    "    eval_dataset=testset_breach,\n",
    "    column_mapping={\"Headline\": \"text\", \"BreachMentioned\": \"label\"},\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "os.makedirs('./classify/setfit/model_breach', exist_ok=True)\n",
    "model_breach._save_pretrained('./classify/setfit/model_breach')\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that we have both 1 and 0 labels in both\n",
    "assert 1 in testset_company['CompanyMentioned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3727/3880213943.py:3: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31037648c7fe4000afd14eee75895519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 8000\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 28:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047c4567a704689805ba62b41efde2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.725}\n"
     ]
    }
   ],
   "source": [
    "# Train CompanyMentioned model\n",
    "# Train setfit\n",
    "trainer = Trainer(\n",
    "    model=model_company,\n",
    "    args=args,\n",
    "    train_dataset=trainset_company,\n",
    "    eval_dataset=testset_company,\n",
    "    column_mapping={\"Headline\": \"text\", \"CompanyMentioned\": \"label\"},\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "os.makedirs('./classify/setfit/model_company', exist_ok=True)\n",
    "model_breach._save_pretrained('./classify/setfit/model_company')\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel is dead. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# saved_model = SetFitModel._from_pretrained('./classify/setfit/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
