{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO_LABEL_PATH        = './testset.csv' #'/gscratch/stf/lleibm/data/headlines.csv'\n",
    "OUT_PATH             = '/gscratch/stf/lleibm/data/gpt_classified.csv'\n",
    "PREFIX               = open('./classify/qlora/prompt_QLora.txt', 'r').read()\n",
    "\n",
    "\"\"\"\n",
    "TO_LABEL_PATH        = './llm_classify/testset.csv' #'/gscratch/stf/lleibm/data/headlines.csv'\n",
    "OUTPUT_DIR           = '/gscratch/stf/lleibm/decilm-7b-headline-qlora'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'BreachMentioned': 'true|True|false|False',\n",
    "    'CompanyMentioned': '\\w{3,}', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './classify/qlora/model'\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": 50,\n",
    "    \"early_stopping\": True,\n",
    "    \"num_beams\": 5,\n",
    "    \"temperature\" : 0.001,\n",
    "    \"do_sample\":True,\n",
    "    \"no_repeat_ngram_size\": 3,\n",
    "    \"repetition_penalty\" : 1.5,\n",
    "    \"renormalize_logits\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL =  \"Deci/DeciLM-7B\"\n",
    "AutoTokenizer.from_pretrained = partial(AutoTokenizer.from_pretrained, trust_remote_code=True)\n",
    "\n",
    "instruction_tuned_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map  = 'auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "merged_model = instruction_tuned_model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "decilm_tuned_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=merged_model,\n",
    "    tokenizer=tokenizer,\n",
    "    **generation_kwargs\n",
    ")\n",
    "\n",
    "def query_batch(prompt_texts:list[str]) -> dict[str: str]:\n",
    "    return {p: x['generated_text'] for p, x in zip(prompt_texts, decilm_tuned_pipeline(prompt_texts))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prompt(row:pd.Series, index=True, columns=True, index_names=None) -> str:\n",
    "    val = list()\n",
    "    if index:\n",
    "        if not index_names:\n",
    "            index_names = row.index.names\n",
    "        if not index_names:\n",
    "            index_names = ['index']\n",
    "        if len(index_names) != len(row.name):\n",
    "            raise Exception(\"Index names must be the same length as the index\")\n",
    "        for i, name in enumerate(index_names):\n",
    "            val.append(f\"{name}: {row.name[i]}\")\n",
    "    \n",
    "    if columns:\n",
    "        for col, value in row.items():\n",
    "            val.append(f\"{col} : {value}\")\n",
    "    return '\\n'.join(val)\n",
    "\n",
    "def construct_fragment(sample, prefix=PREFIX, include_solution=False) -> str:\n",
    "  prompt = \"<s>\"\n",
    "  if prefix: prompt += prefix\n",
    "  prompt += to_prompt(sample, index=True, columns=False, index_names=['Date', 'Publication', 'Headline', 'URL'])\n",
    "  prompt += \" ###> \"\n",
    "  if include_solution:\n",
    "      prompt += to_prompt(sample, index=False, columns=True)\n",
    "      prompt += \"</s>\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_set(frame):\n",
    "    frame = frame.loc[:, ~frame.columns.str.contains('^Unnamed')] # Drop all Unnamed: columns\n",
    "    frame['Headline'] = frame['Headline'].apply(str.strip)\n",
    "    frame.drop_duplicates(subset=['Headline'], inplace=True)\n",
    "    frame.drop_duplicates(subset=['URL'], inplace=True)\n",
    "\n",
    "    frame.set_index(['Date', 'Publication', 'Headline', 'URL'], inplace=True)\n",
    "    frame.fillna(False, inplace=True)\n",
    "    return frame\n",
    "\n",
    "to_label = fix_set(pd.read_csv(TO_LABEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUT_PATH):\n",
    "    os.remove(OUT_PATH)\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "to_label[:0].to_csv(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import chunkify\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "for chunk in chunkify(to_label, 100):    \n",
    "    # Calculate prompts\n",
    "    prompts = {idx: construct_fragment(row) for idx, row in chunk.iterrows()}\n",
    "    \n",
    "    # Chunked query\n",
    "    responses = query_batch([prompt_str for prompt_str in prompts.values()])\n",
    "\n",
    "    # Parse each response\n",
    "    new_rows = pd.DataFrame()\n",
    "    for idx, row in chunk.iterrows():\n",
    "        try:\n",
    "            parsed_response = dict()\n",
    "            response = responses.get(prompts.get(idx, None), \"\")\n",
    "            \n",
    "            # Parse yamllike response\n",
    "            for line in response.split('\\n'):\n",
    "                if ':' in line:\n",
    "                    k,v = line.split(':', 1)\n",
    "                    if k.strip() not in parsed_response:\n",
    "                        parsed_response[k.strip()] = v.strip()\n",
    "            \n",
    "            # Drop all keys in parsed_response that are not in schema\n",
    "            for k in list(parsed_response.keys()):\n",
    "                if k not in schema.keys():\n",
    "                    del parsed_response[k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error Classifying {idx}:\\n{e}\")\n",
    "            \n",
    "            # Find line number of error\n",
    "            print(next((line for line in reversed(traceback.format_exc().split('\\n')) if re.search(r'line \\d+', line)), 'Unknown'))\n",
    "            \n",
    "        # Fill all missing fields with NaN\n",
    "        if not set(schema.keys()) <= set(parsed_response.keys()):\n",
    "            print(f\"Missing keys in response for {idx}: {set(schema.keys()) - set(parsed_response.keys())}\")\n",
    "            print('Filling with NaN')\n",
    "            for k in schema.keys():\n",
    "                if k not in parsed_response:\n",
    "                    parsed_response[k] = np.nan\n",
    "        \n",
    "        # Fill in the row with the parsed response\n",
    "        row_copy = copy(row).to_frame().T\n",
    "        for k,v in parsed_response.items():\n",
    "            if k in row_copy.columns:\n",
    "                row_copy[k] = v\n",
    "        \n",
    "        new_rows = pd.concat([new_rows, row_copy])\n",
    "        \n",
    "    # Save all new rows\n",
    "    if len(new_rows) > 0:\n",
    "        new_rows.reset_index().to_csv(OUT_PATH, mode='a', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.monotonic() - start_time\n",
    "print(f'Processed {len(to_label):,d} records in {elapsed_time/60:.1f} minutes.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
