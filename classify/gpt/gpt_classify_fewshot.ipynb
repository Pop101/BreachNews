{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from copy import copy\n",
    "from dataclasses import dataclass, field\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import embedding_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix path\n",
    "os.chdir(\"/mnt/c/Users/leonl/OneDrive/College/Senior/CSE 481DS/Analysis/classify/gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_PATH   = './trainset.csv'\n",
    "TO_LABEL_PATH        = '../../data/headlines.csv'\n",
    "OUT_PATH             = '../../data/gpt_classified.csv'\n",
    "NUM_EXAMPLES         = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = '2024-06-01'\n",
    "openai.api_base = \"https://llm.leibmann.org/v1\"\n",
    "openai.api_key = \"keyzoned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helpful vLLM commands:\n",
    "- Start server: `python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B --dtype=half --download-dir /projects/bdata/llm_models/`\n",
    "- Base url: `http://localhost:8000/v1`\n",
    "- List models: `curl http://localhost:8000/v1/models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RETRIES          = 3\n",
    "MAX_TOKENS           = 300 # only include response; smaller helps with ratelimiting\n",
    "TEMPERATURE          = 0 # using 0 based on xinyi, down from 1.4 earlier\n",
    "RETRY_SECS           = 5\n",
    "PAUSE_SECS           = 1\n",
    "TIMEOUT_SECS         = 5\n",
    "LLM_DEPLOYMENT       = 'mistral-7b-instruct-v0.2' # 'gpt-4' or 'GPT-4o'. For vLLM use full path\n",
    "\n",
    "if 'localhost' in openai.api_base or 'leibmann.org' in openai.api_base:\n",
    "    PROMPT_COST = 0 # dollars per 1,000 tokens\n",
    "    OUTPUT_COST = 0 # dollars per 1,000 tokens\n",
    "    \n",
    "# input from here https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/\n",
    "elif LLM_DEPLOYMENT == 'GPT-4o':\n",
    "    PROMPT_COST = 0.005 # dollars per 1,000 tokens\n",
    "    OUTPUT_COST = 0.015 # dollars per 1,000 tokens\n",
    "elif LLM_DEPLOYMENT == 'gpt-4':\n",
    "    PROMPT_COST = 0.03 # dollars per 1,000 tokens\n",
    "    OUTPUT_COST = 0.06 # dollars per 1,000 tokens\n",
    "else: raise ValueError(f\"Unknown LLM_DEPLOYMENT: {LLM_DEPLOYMENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounting\n",
    "NUM_TOKENS_PROMPTED  = 0\n",
    "NUM_TOKENS_GENERATED = 0\n",
    "enc = tiktoken.encoding_for_model('gpt-4')\n",
    "def get_num_tokens(s):\n",
    "    return len( enc.encode(s) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helpers for output parsing\n",
    "@dataclass\n",
    "class LLMCol:\n",
    "    \"\"\"Represents a single key-value pair in the dataframe. Used for easy parsing\"\"\"\n",
    "    key: str\n",
    "    allowed_values: set = field(default_factory=set)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.allowed_values = set(map(LLMCol.normalize_str, self.allowed_values))\n",
    "\n",
    "    def to_str(self, value:str):\n",
    "        value = LLMCol.normalize_str(value)\n",
    "        if len(self.allowed_values) > 0 and value not in self.allowed_values:\n",
    "            raise ValueError(f\"Malformed input: Value {value} not in allowed values {self.allowed_values}\")\n",
    "        return f\"{self.key}: {value}\"\n",
    "    \n",
    "    def parse_line(self, line:str) -> dict:\n",
    "        \"\"\"Parse a line of text into a key-value pair, performing the inverse of to_str\"\"\"\n",
    "        try:\n",
    "            k,v = re.split(r'\\W*:\\W*', line, maxsplit=1)\n",
    "            if len(self.allowed_values) > 0 and LLMCol.normalize_str(v) not in self.allowed_values:\n",
    "                raise ValueError(f\"Malformed output: value {v} not in allowed values {self.allowed_values}\")\n",
    "            if k.strip().casefold() != self.key.casefold():\n",
    "                raise ValueError(f\"Malformed output: key {k} does not match expected key {self.key}\")\n",
    "            \n",
    "            return {self.key: v}\n",
    "        except ValueError:\n",
    "            raise ValueError(f'Malformed output for line \"{line}\"')\n",
    "        \n",
    "    def parse_row(self, row:pd.Series|dict) -> str:\n",
    "        \"\"\"Parse a row of a dataframe into a string, performing the inverse of to_str\"\"\"\n",
    "        if isinstance(row, pd.Series):\n",
    "            if self.key not in row.index:\n",
    "                raise ValueError(f\"Key {self.key} not found in row\")\n",
    "            return self.to_str(row[self.key])\n",
    "        elif isinstance(row, dict):\n",
    "            if self.key not in row:\n",
    "                raise ValueError(f\"Key {self.key} not found in row\")\n",
    "            return self.to_str(row[self.key])\n",
    "        else:\n",
    "            raise ValueError(\"Expected a Series or dict\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def normalize_str(s:str) -> str:\n",
    "        return str(s).strip().casefold()\n",
    "    \n",
    "@dataclass\n",
    "class LLMSchema:\n",
    "    \"\"\"Represents a set of LLMRows, used for easy parsing\"\"\"\n",
    "    cols: list[LLMCol]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.cols = tuple(self.cols)\n",
    "        \n",
    "        # Ensure no duplicate keys\n",
    "        keys_found = set()\n",
    "        for col in self.cols:\n",
    "            if col.key in keys_found:\n",
    "                raise ValueError(f\"Duplicate key: {col.key}\")\n",
    "            keys_found.add(col.key)\n",
    "    \n",
    "    @property\n",
    "    def keys(self):\n",
    "        return {c.key for c in self.cols}\n",
    "    \n",
    "    def parse_row(self, row:pd.Series|dict) -> str:\n",
    "        values = list()\n",
    "        for c in self.cols:\n",
    "            try:\n",
    "                values.append(c.parse_row(row))\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return '\\n'.join(values)\n",
    "    \n",
    "    def parse_text(self, text:str) -> list[dict]:\n",
    "        \"\"\"Parse a text block into a dictionary of key-value pairs\"\"\"\n",
    "        output = list()\n",
    "        for block in text.split('\\n\\n'):\n",
    "            lines = re.split(r'(?<=\\n)(?=[^\\n]*:)', block)\n",
    "            pairs = dict()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                for col in self.cols:\n",
    "                    try:\n",
    "                        pairs.update(col.parse_line(line))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            output.append(pairs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the LLM output\n",
    "schema = LLMSchema([\n",
    "    LLMCol('BreachMentioned', {'true', 'false'}),\n",
    "    LLMCol('CompanyMentioned')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>BreachMentioned</th>\n",
       "      <th>CompanyMentioned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Publication</th>\n",
       "      <th>Headline</th>\n",
       "      <th>URL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20160329</th>\n",
       "      <th>Washington Post</th>\n",
       "      <th>You can soon get unlimited data on AT&amp;T U-verse â€” but it comes with a big catch</th>\n",
       "      <th>https://www.washingtonpost.com/news/the-switch/wp/2016/03/29/you-can-soon-get-unlimited-data-on-att-u-verse-but-it-comes-with-a-big-catch/</th>\n",
       "      <td>False</td>\n",
       "      <td>AT&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20180212</th>\n",
       "      <th>Washington Post</th>\n",
       "      <th>Lending by big banks to small businesses hits a record high, study finds</th>\n",
       "      <th>https://www.washingtonpost.com/news/on-small-business/wp/2018/02/12/lending-by-big-banks-to-small-businesses-hits-a-record-high-study-finds/</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20220614</th>\n",
       "      <th>The Guardian</th>\n",
       "      <th>Yellowstone  National park closed after record rain and major flooding</th>\n",
       "      <th>https://www.theguardian.com/us-news/2022/jun/14/yellowstone-national-park-flooding-rain</th>\n",
       "      <td>False</td>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               BreachMentioned  \\\n",
       "Date     Publication     Headline                                           URL                                                                  \n",
       "20160329 Washington Post You can soon get unlimited data on AT&T U-verse... https://www.washingtonpost.com/news/the-switch/...           False   \n",
       "20180212 Washington Post Lending by big banks to small businesses hits a... https://www.washingtonpost.com/news/on-small-bu...           False   \n",
       "20220614 The Guardian    Yellowstone  National park closed after record ... https://www.theguardian.com/us-news/2022/jun/14...           False   \n",
       "\n",
       "                                                                                                                               CompanyMentioned  \n",
       "Date     Publication     Headline                                           URL                                                                  \n",
       "20160329 Washington Post You can soon get unlimited data on AT&T U-verse... https://www.washingtonpost.com/news/the-switch/...             AT&T  \n",
       "20180212 Washington Post Lending by big banks to small businesses hits a... https://www.washingtonpost.com/news/on-small-bu...            False  \n",
       "20220614 The Guardian    Yellowstone  National park closed after record ... https://www.theguardian.com/us-news/2022/jun/14...       Government  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training set\n",
    "train_set = pd.read_csv(TRAINING_DATA_PATH)\n",
    "train_set = train_set.loc[:, ~train_set.columns.str.contains('^Unnamed')] # Drop all Unnamed: columns\n",
    "train_set.drop_duplicates(subset=['Headline'], inplace=True)\n",
    "train_set.drop_duplicates(subset=['URL'], inplace=True)\n",
    "\n",
    "train_set['Headline'] = train_set['Headline'].apply(str.strip)\n",
    "train_set.set_index(['Date', 'Publication', 'Headline', 'URL'], inplace=True)\n",
    "for col in schema.cols:\n",
    "    if col.key in train_set.index.names:\n",
    "        continue\n",
    "    \n",
    "    if col.key not in train_set.columns:\n",
    "        print(f\"WARNING: Column {col.key} not found in training set but specified in schema\")\n",
    "        continue\n",
    "    \n",
    "    #train_set[col.key] = train_set[col.key].apply(lambda x: len(str(x)) > 0 and str(x).lower() not in ['nan','false','0'])\n",
    "\n",
    "train_set.fillna(False, inplace=True)\n",
    "train_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4361388 rules to classify.\n"
     ]
    }
   ],
   "source": [
    "# Load to_label EXACTLY as above\n",
    "to_label = pd.read_csv(TO_LABEL_PATH)\n",
    "\n",
    "to_label.dropna(subset=['Headline'], inplace=True)\n",
    "to_label['Headline'] = to_label['Headline'].apply(str.strip)\n",
    "to_label.set_index(train_set.index.names, inplace=True)\n",
    "\n",
    "# Create columns to label\n",
    "for col in schema.cols:\n",
    "    if col.key in train_set.index.names:\n",
    "        continue\n",
    "    \n",
    "    if col.key not in to_label.columns:\n",
    "        # If we are trying to label a column that doesn't exist, create it\n",
    "        to_label[col.key] = False\n",
    "\n",
    "# Assert that the schema of to_label and train_set match\n",
    "if not set(chain(train_set.index.names, train_set.columns)) <= set(chain(to_label.index.names, to_label.columns)):\n",
    "    raise ValueError(f'To Label schema does not match train set schema! \\n Missing Columns: {set(chain(train_set.index.names, train_set.columns)) - set(chain(to_label.index.names, to_label.columns))}')\n",
    "\n",
    "print(f'Loaded {len(to_label)} rules to classify.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct prompt\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    '\\n'.join(f'{c} : {{{c}}}' for c in chain(train_set.index.names, train_set.columns)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Date : 20120330\\nPublication : CNBC\\nHeadline : MasterCard, Visa Warn of Possible Security Breach\\nURL : http://www.cnbc.com/id/46904168\\nBreachMentioned : Yes\\nCompanyMentioned : MasterCard, Visa'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = train_set.reset_index().astype(str).to_dict(orient='records')\n",
    "example_prompt.format(**examples[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for examples..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 21.0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.monotonic()\n",
    "\n",
    "print('Computing embeddings for examples...', end='')\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,       # This is the list[dict] of examples available to select from.\n",
    "    embedding_pipeline(use_gpu=False),\n",
    "    # AzureOpenAIEmbeddings(\n",
    "    #     deployment = EMBEDDING_DEPLOYMENT\n",
    "    # ),              # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    Chroma,         # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    k=NUM_EXAMPLES  # This is the number of examples to produce.\n",
    ")\n",
    "print(f'done in {time.monotonic()-start_time:.1f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./prompt.txt') as f:\n",
    "    prompt_prefix = f.read()\n",
    "prompt_prefix += '\\n\\nYour answer should follow the format given in the examples:\\n'\n",
    "    \n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector = example_selector,\n",
    "    example_prompt   = example_prompt,\n",
    "    input_variables  = list(train_set.index.names),\n",
    "    prefix           = prompt_prefix,\n",
    "    suffix           = '\\n'.join(chain((f'{c} : {{{c}}}' for c in train_set.index.names))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt formatting configured.\n",
      "Example prompt:\n",
      "Given a news article's headline, determine the following information about it:\n",
      "\n",
      "If the article is about a data breach, make sure you respond with \"BreachMentioned: true\".\n",
      "Otherwise, respond with \"BreachMentioned: false\".\n",
      "\n",
      "If the headline of the article implies the main subject of the article will be a single distinct company, make sure you respond with \"CompanyMentioned: <company name>\", filling in the specific company name.\n",
      "If the article implies that the government will be the main subject of the article, respond with \"CompanyMentioned: government\"\n",
      "If the article mentions two or more companies, or the main subject is not clear, respond with \"CompanyMentioned: false\"\n",
      "\n",
      "Below are some examples of labelled headlines. Follow the format given exactly.\n",
      "\n",
      "Your answer should follow the format given in the examples:\n",
      "\n",
      "\n",
      "Date : 20120330\n",
      "Publication : CNBC\n",
      "Headline : MasterCard, Visa Warn of Possible Security Breach\n",
      "URL : http://www.cnbc.com/id/46904168\n",
      "BreachMentioned : Yes\n",
      "CompanyMentioned : MasterCard, Visa\n",
      "\n",
      "Date : 20151124\n",
      "Publication : CNBC\n",
      "Headline : The biggest threats to your data while  traveling\n",
      "URL : http://www.cnbc.com/2015/11/24/un-hack-me-tips-for-staying-cyber-safe-at-thanksgiving.html\n",
      "BreachMentioned : False\n",
      "CompanyMentioned : False\n",
      "\n",
      "Date : 20150807\n",
      "Publication : Daily Mail\n",
      "Headline : Chinese hackers break in to American Airlines' travel reservation system\n",
      "URL : http://www.dailymail.co.uk/news/article-3189727/Chinese-hackers-target-American-Airlines-global-travel-reservation-company-Sabre-string-earlier-attacks.html\n",
      "BreachMentioned : Yes\n",
      "CompanyMentioned : American Airlines\n",
      "\n",
      "Date : 20180510\n",
      "Publication : New York Post\n",
      "Headline : Equifax now says some passport info was stolen in breach\n",
      "URL : https://nypost.com/2018/05/10/equifax-now-says-some-passport-info-was-stolen-in-breach/\n",
      "BreachMentioned : Yes\n",
      "CompanyMentioned : Equifax\n",
      "\n",
      "Date : 20070201\n",
      "Publication : CNBC\n",
      "Headline : Economic Data Is Still Giving Out Mixed Signals\n",
      "URL : http://www.cnbc.com/id/16922316\n",
      "BreachMentioned : False\n",
      "CompanyMentioned : False\n",
      "\n",
      "Date : 20091211\n",
      "Publication : CNBC\n",
      "Headline : Asia Shares End Higher on Upbeat China Data\n",
      "URL : http://www.cnbc.com/id/34370854\n",
      "BreachMentioned : False\n",
      "CompanyMentioned : False\n",
      "\n",
      "Date : 20120330\n",
      "Publication : CNBC\n",
      "Headline : MasterCard, Visa Warn of Possible Security Breach\n",
      "URL : http://www.cnbc.com/id/46904168\n"
     ]
    }
   ],
   "source": [
    "print('Prompt formatting configured.\\nExample prompt:')\n",
    "\n",
    "print(\n",
    "    prompt.format(\n",
    "        **examples[5]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-19e7e21f-fe12-46cb-8d2e-489d173ed503 at 0x7f6c251cc950> JSON: {\n",
       "  \"id\": \"chatcmpl-19e7e21f-fe12-46cb-8d2e-489d173ed503\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1728781755,\n",
       "  \"model\": \"mistral-7b-instruct-v0.2\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \" Date : 20170915\\nPublication : Reuters\\nHeadline : Hackers steal data from credit reporting agency Equifax, potentially impacting 143 million people\\nURL : http://www.reuters.com/article/us-equifax-cybersecurity-idUSKCN1BZ256\\nBreachMentioned : Yes\\nCompanyMentioned : Equifax\",\n",
       "        \"role\": \"assistant\"\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 806,\n",
       "    \"completion_tokens\": 96,\n",
       "    \"total_tokens\": 902\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model         = LLM_DEPLOYMENT,\n",
    "    #deployment_id = LLM_DEPLOYMENT,\n",
    "    messages      = [{'role': 'user', 'content': prompt.format(**examples[5])}],\n",
    "    max_tokens    = MAX_TOKENS,\n",
    "    temperature   = TEMPERATURE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(prompt):\n",
    "    current_tries = 1\n",
    "    \n",
    "    while current_tries <= MAX_RETRIES:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model         = LLM_DEPLOYMENT,\n",
    "                #deployment_id = LLM_DEPLOYMENT,\n",
    "                messages      = [{'role': 'user', 'content': prompt}],\n",
    "                max_tokens    = MAX_TOKENS,\n",
    "                temperature   = TEMPERATURE,\n",
    "            )\n",
    "            break\n",
    "        except openai.error.RateLimitError:\n",
    "            time.sleep(.1*4**current_tries)\n",
    "            current_tries += 1\n",
    "        except Exception as e:\n",
    "            if 'The response was filtered due to the prompt' in str(e):\n",
    "                raise RuntimeError('Prompt was filtered.')\n",
    "            \n",
    "            print('\\tError from OpenAI:', str(e))\n",
    "            print('\\tRetrying...')\n",
    "            time.sleep(RETRY_SECS)\n",
    "            current_tries += 1 \n",
    "    \n",
    "    if current_tries > MAX_RETRIES:\n",
    "        raise RuntimeError('No valid response from OpenAI.')\n",
    "    \n",
    "    global NUM_TOKENS_PROMPTED\n",
    "    global NUM_TOKENS_GENERATED\n",
    "    \n",
    "    NUM_TOKENS_PROMPTED  += get_num_tokens( prompt )\n",
    "    NUM_TOKENS_GENERATED += get_num_tokens( response['choices'][0]['message']['content'] )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29ad169990d4997801bca42861b7cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4361388 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys in response for (20070101, 'New York Times', 'Rush to Hang Hussein Was  Questioned', 'http://www.nytimes.com/2007/01/01/world/middleeast/01iraq.html?hp&ex=1167714000&en=85dae91ed8178e3a&ei=5094&partner=homepage'): {'BreachMentioned', 'CompanyMentioned'}\n",
      "Filling with NaN\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leonl\\OneDrive\\College\\Senior\\CSE 481DS\\Analysis\\classify\\gpt\\gpt_classify_fewshot.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/gpt/gpt_classify_fewshot.ipynb#X30sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m parsed_response\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/gpt/gpt_classify_fewshot.ipynb#X30sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             to_label\u001b[39m.\u001b[39mloc[idx,k] \u001b[39m=\u001b[39m to_bool(v)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/gpt/gpt_classify_fewshot.ipynb#X30sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     to_label\u001b[39m.\u001b[39;49mto_csv(OUT_PATH)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/leonl/OneDrive/College/Senior/CSE%20481DS/Analysis/classify/gpt/gpt_classify_fewshot.ipynb#X30sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3968\u001b[0m     path_or_buf,\n\u001b[1;32m   3969\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3970\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3971\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3972\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3973\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3974\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3975\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3976\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3977\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3978\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3979\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3980\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3981\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3982\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3983\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3984\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/io/formats/csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/io/formats/csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    274\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[0;32m--> 275\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/io/formats/csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[1;32m    312\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/io/formats/csvs.py:323\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    320\u001b[0m res \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_get_values_for_csv(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[1;32m    321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(res\u001b[39m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m--> 323\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_index[slicer]\u001b[39m.\u001b[39;49m_get_values_for_csv(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_number_format)\n\u001b[1;32m    324\u001b[0m libwriters\u001b[39m.\u001b[39mwrite_csv_rows(\n\u001b[1;32m    325\u001b[0m     data,\n\u001b[1;32m    326\u001b[0m     ix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter,\n\u001b[1;32m    330\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/indexes/multi.py:1396\u001b[0m, in \u001b[0;36mMultiIndex._get_values_for_csv\u001b[0;34m(self, na_rep, **kwargs)\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[39m# go through the levels and format them\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[39mfor\u001b[39;00m level, level_codes \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlevels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcodes):\n\u001b[0;32m-> 1396\u001b[0m     level_strs \u001b[39m=\u001b[39m level\u001b[39m.\u001b[39;49m_get_values_for_csv(na_rep\u001b[39m=\u001b[39;49mna_rep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1397\u001b[0m     \u001b[39m# add nan values, if there are any\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m     mask \u001b[39m=\u001b[39m level_codes \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/indexes/base.py:1478\u001b[0m, in \u001b[0;36mIndex._get_values_for_csv\u001b[0;34m(self, na_rep, decimal, float_format, date_format, quoting)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_values_for_csv\u001b[39m(\n\u001b[1;32m   1470\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1471\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m     quoting\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1477\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mobject_]:\n\u001b[0;32m-> 1478\u001b[0m     \u001b[39mreturn\u001b[39;00m get_values_for_csv(\n\u001b[1;32m   1479\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values,\n\u001b[1;32m   1480\u001b[0m         na_rep\u001b[39m=\u001b[39;49mna_rep,\n\u001b[1;32m   1481\u001b[0m         decimal\u001b[39m=\u001b[39;49mdecimal,\n\u001b[1;32m   1482\u001b[0m         float_format\u001b[39m=\u001b[39;49mfloat_format,\n\u001b[1;32m   1483\u001b[0m         date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   1484\u001b[0m         quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   1485\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/indexes/base.py:7864\u001b[0m, in \u001b[0;36mget_values_for_csv\u001b[0;34m(values, date_format, na_rep, quoting, float_format, decimal)\u001b[0m\n\u001b[1;32m   7861\u001b[0m     \u001b[39mreturn\u001b[39;00m new_values\n\u001b[1;32m   7863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 7864\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[1;32m   7865\u001b[0m     itemsize \u001b[39m=\u001b[39m writers\u001b[39m.\u001b[39mword_len(na_rep)\n\u001b[1;32m   7867\u001b[0m     \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m _dtype_obj \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m quoting \u001b[39mand\u001b[39;00m itemsize:\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:178\u001b[0m, in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(obj: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mbool_] \u001b[39m|\u001b[39m NDFrame:\n\u001b[1;32m    102\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna(obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:207\u001b[0m, in \u001b[0;36m_isna\u001b[0;34m(obj, inf_as_na)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (np\u001b[39m.\u001b[39mndarray, ABCExtensionArray)):\n\u001b[0;32m--> 207\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna_array(obj, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    208\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, ABCIndex):\n\u001b[1;32m    209\u001b[0m     \u001b[39m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m obj\u001b[39m.\u001b[39m_can_hold_na:\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:292\u001b[0m, in \u001b[0;36m_isna_array\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    290\u001b[0m     result \u001b[39m=\u001b[39m _isna_recarray_dtype(values, inf_as_na\u001b[39m=\u001b[39minf_as_na)\n\u001b[1;32m    291\u001b[0m \u001b[39melif\u001b[39;00m is_string_or_object_np_dtype(values\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m--> 292\u001b[0m     result \u001b[39m=\u001b[39m _isna_string_dtype(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    293\u001b[0m \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmM\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39m# this is the NaT pattern\u001b[39;00m\n\u001b[1;32m    295\u001b[0m     result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mview(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m==\u001b[39m iNaT\n",
      "File \u001b[0;32m~/miniforge3/envs/openai/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:313\u001b[0m, in \u001b[0;36m_isna_string_dtype\u001b[0;34m(values, inf_as_na)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39min\u001b[39;00m {\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m--> 313\u001b[0m         result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39;49misnaobj(values, inf_as_na\u001b[39m=\u001b[39;49minf_as_na)\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         \u001b[39m# 0-D, reached via e.g. mask_missing\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         result \u001b[39m=\u001b[39m libmissing\u001b[39m.\u001b[39misnaobj(values\u001b[39m.\u001b[39mravel(), inf_as_na\u001b[39m=\u001b[39minf_as_na)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.monotonic()\n",
    "\n",
    "to_bool = lambda x: len(str(x)) > 0 and str(x).lower() not in ['nan','false','0']\n",
    "\n",
    "for idx, row in tqdm(to_label.iterrows(), total=to_label.shape[0]):\n",
    "    prompt_args = dict(zip(to_label.index.names, idx))\n",
    "    prompt_str = prompt.format(**{k: str(v) for k, v in chain(prompt_args.items(), row.items())})\n",
    "    parsed_response = dict()\n",
    "    \n",
    "    try:\n",
    "        response = query(prompt_str)\n",
    "        \n",
    "        response_str = response['choices'][0]['message']['content']\n",
    "        response_str = response_str.replace(prompt_str, '').strip()\n",
    "        response_str = \"Prescriptive : \" + response_str if not ':' in response_str.splitlines()[0] else response_str\n",
    "        response_str = response_str[response_str.find('Prescriptive : '):]\n",
    "        \n",
    "        parsed_response = schema.parse_text(response_str)[0]\n",
    "        \n",
    "        # Drop all keys in parsed_response that are not in schema\n",
    "        for k in list(parsed_response.keys()):\n",
    "            if k not in schema.keys:\n",
    "                del parsed_response[k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error Classifying {idx}:\\n{e}\")\n",
    "        \n",
    "        # Find line number of error\n",
    "        print(next((line for line in reversed(traceback.format_exc().split('\\n')) if re.search(r'line \\d+', line)), 'Unknown'))\n",
    "        \n",
    "    # Fill all missing fields with NaN\n",
    "    if not set(schema.keys) <= set(parsed_response.keys()):\n",
    "        print(f\"Missing keys in response for {idx}: {set(schema.keys) - set(parsed_response.keys())}\")\n",
    "        print('Filling with NaN')\n",
    "        for k in schema.keys:\n",
    "            if k not in parsed_response:\n",
    "                parsed_response[k] = np.nan\n",
    "    \n",
    "    # Fill in the row with the parsed response\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for k,v in parsed_response.items():\n",
    "            to_label.loc[idx,k] = to_bool(v)\n",
    "    to_label.to_csv(OUT_PATH)\n",
    "    \n",
    "elapsed_time = time.monotonic() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_response = schema.parse_text(response_str)[0]\n",
    "        \n",
    "# Drop all keys in parsed_response that are not in schema\n",
    "for k in list(parsed_response.keys()):\n",
    "    if k not in schema.keys:\n",
    "        del parsed_response[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prescriptive : False\\nRestrictive : True\\nPost Content : False\\nPost Format : False\\nUser-Related : False\\nNot a Rule : False\\nSpam, Low Quality, Off-Topic, and Reposts : False\\nPost Tagging & Flairing : False\\nPeer Engagement : True\\nLinks & External Content : False\\nImages : False\\nCommercialization : False\\nIllegal Content : False\\nDivisive Content : False\\nRespect for Others : False\\nBrigading : False\\nBan Mentioned : False\\nKarma/Score Mentioned : True'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3,000 records in 99.2 minutes.\n",
      "4,432,262 tokens prompted, 434,489 generated.\n",
      "Total run cost ~ $28.68\n"
     ]
    }
   ],
   "source": [
    "print(f'Processed {len(to_label):,d} records in {elapsed_time/60:.1f} minutes.')\n",
    "\n",
    "print(f'{NUM_TOKENS_PROMPTED:,d} tokens prompted, {NUM_TOKENS_GENERATED:,d} generated.')\n",
    "\n",
    "run_cost = (NUM_TOKENS_PROMPTED/1000 * PROMPT_COST)+(NUM_TOKENS_GENERATED/1000 * OUTPUT_COST)\n",
    "\n",
    "print(f'Total run cost ~ ${run_cost:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../cost_tracking.jsonl', 'a') as f:\n",
    "    costs = {\n",
    "        'date'                  : datetime.date.today().isoformat(),\n",
    "        'time'                  : datetime.datetime.now().strftime('%H:%M'),\n",
    "        'tokens_prompted'       : NUM_TOKENS_PROMPTED,\n",
    "        'tokens_generated'      : NUM_TOKENS_GENERATED,\n",
    "        'run_cost_usd'          : run_cost,\n",
    "        'elapsed_seconds'       : elapsed_time,\n",
    "        'records_processed'     : len(to_label),\n",
    "    }\n",
    "    \n",
    "    f.write(json.dumps(costs)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
